---
title: "CommonRandomNumber3"
output:
  html_document: default
  pdf_document: default
date: "2025-02-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(matlib)
library(cubature)
library(latex2exp)
library(ggplot2)
library(MCMCpack)
library(coda)
```

## Bayesian Regression Gibbs Sampler

Numerical example
```{r}
# Data is obtained from the general linear models textbook by Dobson
#Table 6.3 Carbohydrate, age, relative weight and protein for twenty male insulin dependent diabetics; for units, see text (data from K. Webb, personal communication).

df <- c(33, 33, 100, 14,
40, 47, 92, 15,
37, 49, 135, 18,
27, 35, 144, 12,
30, 46, 140, 15,
43, 52, 101, 15,
34, 62, 95, 14,
48, 23, 101, 17,
30, 32, 98, 15,
38, 42, 105, 14,
50, 31, 108, 17,
51, 61, 85, 19,
30, 63, 130, 19,
36, 40, 127, 20,
41, 50, 109, 15,
42, 64, 107, 16,
46, 56, 117, 18,
24, 61, 100, 13,
35, 48, 118, 18,
37, 28, 102, 14)

df <- matrix(df, nrow = 20, byrow=TRUE)

Y <- df[,1]
X <- cbind(1,df[,-1])
n <- length(Y)
p <- dim(X)[2]

df <- data.frame(df)
colnames(df) <- c("carbs", "age", "weight", "protein")
```

```{r}
# setting priors: beta ~ N(b_0,E_0) sigma^2 ~ Inv-Chi^2(v_0, c_0)
b_0 <- rep(0,4)
E_0 <- diag(4)
v_0 <- 6
c_0 <- 140
```


```{r}
L_int <- function(x){ 
  b <- c(x[1], x[2], x[3], x[4])
  num <- exp(- 0.5*t(b-b_0) %*% inv(E_0) %*% (b-b_0) )
  denom <- (v_0*c_0/2 + t(Y-(X %*% b)) %*% (Y-(X %*% b))/2)^((n+v_0)/2)
  z = num/denom
  return(z)
  }
```

```{r}
# new version
#x <- c(0.1,0.1,0.1,0.1,0.1)
# x <- optim(x, f)$par
# x[5] <- 5
# x[1] <- 0
x <- c(0, -0.01641627, -0.06559585, 2.64055105)
#x <- c(0.1,0.1,0.1,0.1)
#x <- beta_hat
int_f <- adaptIntegrate(L_int, lowerLimit = x-c(1.35,0.05,0.35,0.35), upperLimit = x+c(2.7,0.35,0.35,0.35))
L <- int_f$integral
L
```

```{r}
beta_hat <- inv(t(X)%*%X)%*%t(X)%*%Y
mle <- t(Y-X%*%beta_hat)%*%(Y-X%*%beta_hat)
num <- 16*gamma((n+v_0)/2-2)*(2*pi)^(p/2)
den <- gamma((n+v_0)/2)*mle^2*(v_0*c_0/2)^((n+v_0)/2-2)*exp(2)
C <- num/den
C[1,1]

K <- C[1,1]/L
K

tvbound <- (n+v_0)^2/(2*v_0*c_0)
tvbound
K*tvbound
```
```{r}
alpha <- (n+v_0)/2-2
beta <- v_0*c_0/2
alpha
beta
```

The following is function that generates $\sigma^2_{n+1}$ given $\sigma^2_n$, equation 19.

```{r}
b_hat <- inv(t(X) %*% X) %*% t(X) %*%  Y
invE_0 <- inv(E_0)
nextIt <- function(o, Z, G){
  V <- t(X) %*% X/o + inv(E_0)
  # eigenV <- eigen(V)
  # Q <- eigenV$vectors
  # L <- diag(eigenV$values)
  # Vinv12 <-  Q %*% inv(sqrt(L)) %*% inv(Q)
  #Vinv <-  Q %*% inv(L) %*% inv(Q)
  Vinv <-  solve(V)
  eigenV <- eigen(Vinv)
  Q <- eigenV$vectors
  L <- diag(eigenV$values)
  Vinv12 <-  Q %*% inv(sqrt(L)) %*% inv(Q)
  #b_tilde <- Vinv %*% (t(X) %*% X %*% b_hat/o + inv(E_0) %*% b_0)
  b_tilde <- Vinv %*% (t(X) %*% Y/o + inv(E_0) %*% b_0)
  #W <- X %*% b_tilde - Y + X %*% Vinv12 %*% Z
  W <- X %*% (b_tilde + Vinv12 %*% Z) - Y
  o1 <- (v_0*c_0/2 +(t(W) %*% W)/2)/G
  return(o1)
}

```

The following is a function that generates $\beta_n$ from $\sigma_n^2$
```{r}
BetaIt <- function(o, Z){
  eigenV <- eigen(t(X) %*% X/o + inv(E_0))
  Q <- eigenV$vectors
  L <- diag(eigenV$values)
  Vinv <-  Q %*% inv(L) %*% inv(Q)
  Vinv12 <-  Q %*% inv(sqrt(L)) %*% inv(Q)
  b_tilde <- Vinv %*% (t(X) %*% X %*% b_hat/o + inv(E_0) %*% b_0)
  #b_tilde <- Vinv %*% (t(X) %*% Y/o + inv(E_0) %*% b_0)
  b <- b_tilde + Vinv12 %*% Z
  return(b)
}
```

Now we apply the common random number technique to generate an estimate of $E[|X_k-Y_k|]$, $N=100$ $I=10000$ and $X_0,X'_0\sim \Gamma^{-1}(\alpha',\beta')=\Gamma^{-1}(11,420)$

```{r}
# I = 10000 # no of simulations
# J = 100 # no of iterations
# # I=10
# # J=10
# diff <- matrix(0, I, J)
# diffB <- matrix(0, I, J)
# it1 <- matrix(0, I, J)
# it2 <- matrix(0, I, J)
# for(i in 1:I){
#   it <- matrix(0, ncol=2, nrow=J)
# 
#   # it[1,] <- 1/rgamma(2, shape = alpha, rate = beta)
#   it[1,] <- c(1/rgamma(1, shape = alpha, rate = beta), 100)
# 
#   beta1 <- rnorm(p, 0, 1)
#   beta2 <- rnorm(p, 0, 1)
#   diffB[i,1] <- sum(abs(beta1-beta2))
#   for(j in 2:J){
#     Z <- rnorm(p, 0, 1)
#     G <- rgamma(1, shape = alpha, rate =1)
#     it[j,1] <- nextIt(it[j-1,1], Z, G)
#     it[j,2] <- nextIt(it[j-1,2], Z, G)
# 
#     # # Calculate beta
#     # Z <- rnorm(p, 0, 1)
#     # beta1 <- BetaIt(it[j,1], Z)
#     # beta2 <- BetaIt(it[j,2], Z)
#     # diffB[i,j] <- sum(abs(beta1-beta2))
#   }
# 
#   diff[i,] <- abs(it[,1]-it[,2])
#   #it1[i,] <- it[,1]
#   #it2[i,] <- it[,2]
# }
```

```{r}
# diff_df <- data.frame(t(diff), iter_no = 1:J)
# diff_df <- diff_df %>%
#   pivot_longer(cols = starts_with("X"), names_to = "sim_no", values_to = "val")
# 
# diffB_df <- data.frame(t(diffB), iter_no = 1:J)
# diffB_df <- diffB_df %>%
#   pivot_longer(cols = starts_with("X"), names_to = "sim_no", values_to = "val")
# 
# diff_df <- diff_df %>% left_join(diffB_df, by=c("iter_no","sim_no"))
# names(diff_df) <- c("iter_no", "sim_no", "o", "b")
# diff_df <- diff_df %>% mutate(val = o+b)
```

```{r}
# save(diff_df, file = "diff_df_BayesReg.RData")
```

```{r}
load('diff_df_BayesReg.RData')
```

```{r}
r <- 5
K^(1/r)
tvbound*(K)^(1/r)
```
```{r}
diff_df
```

```{r}
diff_sum <- diff_df %>%
  group_by(iter_no) %>% 
  summarize(mean_val = mean(val^r), var_val = var(val^r)) %>% 
  mutate(lci = tvbound*K^(1/r)*(mean_val-qnorm(0.975)*sqrt(var_val/10000))^(1/r), 
         uci = tvbound*K^(1/r)*(mean_val+qnorm(0.975)*sqrt(var_val/10000))^(1/r),
         bnd_est = tvbound*K^(1/r)*(mean_val)^(1/r))
```
```{r}
diff_sum %>% filter(bnd_est<0.01) %>% 
  filter(iter_no == min(iter_no))
```

```{r}
diff_sum %>% 
  dplyr::select(iter_no, bnd_est, uci) %>% 
  pivot_longer(cols = bnd_est:uci, names_to = "Bound", values_to = "value") %>% 
  filter(iter_no>2) %>% 
  ggplot(aes(x = iter_no, y = value, color = Bound), ylim=c(0,10^6)) + 
  scale_color_manual(labels = c("Estimate", "97.5% upper C.I."), values = c("bnd_est" = "black", "uci" = "red")) +
  geom_line() + 
  scale_y_log10() +
  theme_bw(base_size=13) + theme(legend.position= 'bottom') +
  geom_hline(yintercept=0.01, linetype=2) +
  xlab("n=iteration") + ylab(TeX("Logscale of $||L(\\sigma^2_n,\\beta_n)-\\pi||_{TV}$")) +
  geom_text(aes(100,0.0001,label = 'y=0.01', vjust = -1, colour = 'black')) 
```

## Bound comparison

```{r}
# # generate overdispersed simulations
# init_val <- seq(5,1000, by =50)
# I = length(it_init)
# J = 200
# 
# it <- data.frame(cbind(sort(rep(1:I,J)), rep(1:J,I), matrix(0, I*J, 1)))
# colnames(it) <- c("sim_no","it", "val")
# 
# for(i in 1:I){
# 
#   it[it$it==1 & it$sim_no==i, "val"] <- init_val[i]
# 
#   for(j in 2:J){
#     Z <- rnorm(p, 0, 1)
#     G <- rgamma(1, shape = alpha, rate =1)
#     it[it$it==j & it$sim_no==i,"val"] <- nextIt(it[it$it==j-1 & it$sim_no==i, "val"], Z, G)
#   }
# }
# save(it, file = "it_BayesReg.RData")
```

```{r}
# load('it_BayesReg.RData')
```

```{r}
# # Calculation of the Gelman Rubin diagnostic.
# gr_diag <- seq(4,J,2) # values must be less than M+1
# for(j in 1:length(gr_diag)){
#   it_J <- it %>%
#     filter(it <= gr_diag[j])
# 
#   it_J_i <- it_J %>% filter(sim_no == 1) %>%
#     dplyr::select(val)
#   it_list <- list(mcmc(it_J_i))
#   for(i in 2:I){
#     it_J_i <- it_J %>% filter(sim_no == i) %>% dplyr::select(val)
#     it_list <- append(it_list, list(mcmc(it_J_i)))
#   }
#   gr <- gelman.diag(it_list)
#   gr_diag[j] <- gelman.diag(it_list)$psrf[1,1]
# }
# gr_diag <- data.frame(it=seq(4,J,2)/2,gr=gr_diag)
# gr_diag
# save(gr_diag, file = "gr_diag_BayesReg.RData")
```

```{r}
load('gr_diag_BayesReg.RData')
```

```{r}
gr_diag %>% 
  ggplot(aes(x = it)) + 
  geom_line(aes(y = gr), size=1) + 
  theme(legend.position = "none") + 
  theme_bw(base_size = 13) + 
  xlab("n=iteration") + ylab(TeX("Shrink factor")) 
```

```{r}
first_hit <- gr_diag %>%
  filter(gr<=1.05) %>%
  summarise(first_hit=min(it))

gr_diag %>%
  filter(it==first_hit$first_hit) 

```

```{r}
# # Graphing a traceplot, sigma^2_0=100
# I = 10
# J = 100
# 
# it1 <- matrix(0, I, J)
# for(i in 1:I){
#   it <- matrix(0, ncol=2, nrow=J)
# 
#   it[1,] <- 100
# 
#   for(j in 2:J){
#     Z <- rnorm(p, 0, 1)
#     G <- rgamma(1, shape = alpha, rate =1)
#     it[j,1] <- nextIt(it[j-1,1], Z, G)
#   }
# 
#   it1[i,] <- it[,1]
# }
# 
# save(it1, file = "it1_BayesReg.RData")
```

```{r}
load('it1_BayesReg.RData')
```

```{r}
data.frame(sim_no=1:10,it1) %>% 
  pivot_longer(cols = starts_with("X"),
    names_to = "it",
    names_prefix = "X",
    values_to = "val") %>% 
  mutate(sim_no=as.character(sim_no), it=as.numeric(it)) %>% 
  ggplot(aes(x = it)) + 
  geom_line(aes(y = val, col=sim_no)) + 
  theme(legend.position = "none") + 
  theme_bw(base_size = 13) + 
  xlab("n=iteration") + ylab(TeX("Value of $\\sigma^2_n$")) + theme(legend.position = "none")
```
